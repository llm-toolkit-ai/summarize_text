import os
from dotenv import load_dotenv
import requests
import json
load_dotenv()

def openai_summarize_text(api_key, text, model="gpt-4", temperature=0.7, max_tokens=100, stop=None):
    """
    Summarizes a given text using the OpenAI API.

    Parameters:
    api_key (str): The API key for accessing the OpenAI API.
    text (str): The text to be summarized.
    model (str): The model to use for summarization (default is "gpt-4").
    temperature (float): Sampling temperature to control the creativity of the model (default is 0.7).
    max_tokens (int): The maximum number of tokens in the generated summary (default is 100).
    stop (str or list): Optional stop sequence to end the generation.

    Returns:
    str: Summary of the text generated by the OpenAI API.
    """
    task_description = "Summarize the following text."

    prompt_content = f"""
    {task_description}

    Text: {text}
    Summary:
    """

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt_content}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stop": stop
    }

    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        response_json = response.json()
        generated_summary = response_json["choices"][0]["message"]["content"].strip()
        return generated_summary
    else:
        return f"Error {response.status_code}: {response.text}"


def anthropic_summarize_text(api_key, text, model="claude-3-5-sonnet-20240620", max_tokens=1024, temperature=0.7):
    """
    Summarizes a given text using the Anthropic API.

    Parameters:
    api_key (str): The API key for accessing the Anthropic API.
    text (str): The text to be summarized.
    model (str): The model to use for text generation (default is "claude-3-5-sonnet-20240620").
    max_tokens (int): The maximum number of tokens in the generated response (default is 1024).
    temperature (float): Sampling temperature to control the creativity of the model (default is 0.7).

    Returns:
    str: Summary generated by the Anthropic API.
    """
    url = "https://api.anthropic.com/v1/messages"
    
    headers = {
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json"
    }

    data = {
        "model": model,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "messages": [
            {"role": "user", "content": f"Please summarize the following text:\n\n{text}"}
        ]
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        response_json = response.json()
        generated_text = response_json["content"][0]["text"].strip()
        return generated_text
    else:
        return f"Error {response.status_code}: {response.text}"


def run_mistral(api_key, user_message, model="mistral-medium-latest"):
    url = "https://api.mistral.ai/v1/chat/completions"
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": user_message}
        ],
        "temperature": 0.7,
        "top_p": 1.0,
        "max_tokens": 512,
        "stream": False,
        "safe_prompt": False,
        "random_seed": 1337
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        response_json = response.json()
        return response_json["choices"][0]["message"]["content"].strip()
    else:
        return f"Error {response.status_code}: {response.text}"

def mistral_summarize_text(api_key, text, model="mistral-medium-latest"):
    """
    Summarizes a given text using the Mistral API.

    Parameters:
    api_key (str): The API key for accessing the Mistral API.
    text (str): The text to be summarized.
    model (str): The model to use for text generation (default is "mistral-medium-latest").

    Returns:
    str: Summary generated by the Mistral API.
    """
    user_message = f"Please summarize the following text:\n\n{text}"
    return run_mistral(api_key, user_message, model=model)

